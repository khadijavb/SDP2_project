from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import docx
import nltk
import re
import numpy as np
import pypandoc
docxFilename = 'C:/Users/ACER PREDATOR/Desktop/d1.docx'
output = pypandoc.convert_file(docxFilename, 'plain', outputfile="C:/Users/ACER PREDATOR/Desktop/test.txt")
assert output == ""
f = open("C:/Users/ACER PREDATOR/Desktop/test.txt", "r")
fullText1= f.read()

res1 = nltk.word_tokenize(fullText1.replace(".","").replace(",",""))
stopWords = set(stopwords.words("english")) 
freqTable = dict() 
for word in res1: 
    word = word.lower() 
    if word in stopWords: 
        continue
    if word in freqTable: 
        freqTable[word] += 1
    else: 
        freqTable[word] = 1


sentences = sent_tokenize(fullText1) 
sentenceValue = dict() 
for sentence in sentences: 
    for word, freq in freqTable.items(): 
        if word in sentence.lower(): 
            if sentence in sentenceValue: 
                sentenceValue[sentence] += freq 
            else: 
                sentenceValue[sentence] = freq 

sumValues = 0
for sentence in sentenceValue: 
    sumValues += sentenceValue[sentence] 

average = int(sumValues / len(sentenceValue)) 

summary = '' 
for sentence in sentences: 
    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)): 
        summary += " " + sentence 

print(summary)
