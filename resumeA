import spacy
from spacy import displacy
NER = spacy.load('en_core_web_lg')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import docx
import nltk
import numpy as np
import pypandoc  
lemmatizer = WordNetLemmatizer()

def read_experience_lines(filename):
    experience_lines = []
    with open(filename) as f:
        for line in f:
            if line.strip() == "Experience":
                # read the next lines until an empty line is encountered
                next_line = next(f).strip()
                while next_line != "":
                    experience_lines.append(next_line)
                    next_line = next(f).strip()
                break
    return experience_lines

ex=read_experience_lines("C:/Users/ACER PREDATOR/Desktop/test.txt")

sw = stopwords.words('english')
X_set=[]; 

res = [sub.split() for sub in ex]

# remove stop words from the string
for i in range(len(res)):
    for word in res[i]:
        if word.lower() not in sw:
            X_set.append(lemmatizer.lemmatize(word.lower()))
            
X_set = list(map(lambda x: x.replace('.', ' '),X_set ))
X_set = list(map(lambda x: x.replace(',', ' '),X_set ))

ex_str=" ".join(X_set)
cv=[]
text1= NER(ex_str)
for word in text1.ents:
    cv.append(word.text.lower())
    print(word.text,word.label_)

#read_degree_lines("C:/Users/ACER PREDATOR/Desktop/test.txt")
#read_skills_lines("C:/Users/ACER PREDATOR/Desktop/test.txt")